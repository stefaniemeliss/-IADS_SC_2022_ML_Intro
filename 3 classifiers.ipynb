{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Scikit Learn: Linear Classifiers and the SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Michael Fairbank\n",
    "\n",
    "Based loosely on the [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the 'iris' dataset\n",
    "\n",
    "The dataset is loaded and prepared in the same way as we saw with the diabetes dataset:\n",
    "\n",
    "1. Load the dataset.\n",
    "2. Print the DESCR field to learn about the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IRIS data sets consists of 3 different types of irises: (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "\n",
    "See [here](https://en.wikipedia.org/wiki/Iris_flower_data_set) for more information on this dataset.\n",
    "\n",
    "## Target column\n",
    "Append the target column into the dataframe, for convenience\n",
    "- print the first 5 rows of the dataframe using \"head\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=iris.data\n",
    "df[\"label\"]=iris.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data\n",
    "\n",
    "\n",
    "It's a bit difficult to visualise the iris input-feature space, since there are 4 features for each flower (and we can't plot in 4D).  As a compromise, the two plots below just visualise two features at a time.  The colour of each point plotted indicates the flower label. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X = df[[\"sepal length (cm)\",\"sepal width (cm)\"]].values  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\")\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "X = df[[\"petal length (cm)\",\"petal width (cm)\"]].values  # we only take the last two features.\n",
    "y = iris.target\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the iris dataset is pretty small - just 150 rows.\n",
    "\n",
    "- There are 4 input features, and 1 data-label column.\n",
    "\n",
    "- Unlike the previous example, which was a \"regression\" task, this task is a \"classificatin task\". \n",
    "\n",
    "- Each target is a discrete label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the \"labels\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "- By studying iris.target.values array above, how many different types of iris flowers are in this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the data and splitting into test and training sets\n",
    "\n",
    "The data above is not shuffled.  Also, we shouldn't use *all* of our data to train a ML system - we should hold some back (\"the test set\" or \"validation set\") to see how well our ML system generalises on unseen (out-of-sample) data. \n",
    "\n",
    "We can do this using the sklearn.model_selection.train_test_split function, which shuffles and splits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target.values, test_size=0.2)\n",
    "print(\"Training data:\",x_train.shape,y_train.shape)\n",
    "print(\"Test data:\",x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- How many data samples are in our training and test sets, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the data is shuffled now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Linear classifier\n",
    "\n",
    "In the case of the iris dataset, the task is to predict, given an the flower's attributes (lengths etc), which iris type it represents. We are given samples of each of the 3 possible classes (the 3 types of iris flowers). These can be used to fit an estimator to predict the class an unseen example belongs to.\n",
    "\n",
    "In scikit-learn, an estimator for classification is a Python object that implements the methods `fit(X, y)` and `predict(T)`. If you remember in the last lab we saw that the preprocessing classes had the methods `fit(X, y)` and `transform(T)`. We will see the differences later.\n",
    "\n",
    "An example of an estimator is the class `klearn.linear_model.SGDClassifier` that implements a Linear Classifier, which we will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "linear_clf=SGDClassifier(max_iter=1000, tol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our estimator instance `linear_clf`, as it is a classifier. **It now must be fitted to the model, that is, it must learn from the data**. This is done by passing our training set to the `fit` method. Note that we only show the *training data* to the classifer - the *test* dataset is kept secret from it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can predict new values. In particular, we can ask to the classifier to recognise all of the data in our test dataset of iris flowers, which we have not used to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_clf.predict(x_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics for classifiers\n",
    "\n",
    "What percentage of the above \"predictions\" are correct?  This is what we call \"accuracy\".\n",
    "\n",
    "We can use the sklearn.metrics.accuracy_score to work out the accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_test=accuracy_score(y_true=y_test, y_pred=linear_clf.predict(x_test))\n",
    "acc_train=accuracy_score(y_true=y_train, y_pred=linear_clf.predict(x_train))\n",
    "print(\"test set accuracy\",acc_test)\n",
    "print(\"train set accuracy\",acc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "A confusion tells you how many of each actual category are classified correctly or misclassified as other categories:\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred=linear_clf.predict(x_test))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall metrics\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Precision_and_recall for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(y_test, y_pred=linear_clf.predict(x_test))\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Support Vector Machine (SVM) classifier\n",
    "\n",
    "Now we will try the same exercise again using a support-vector machine (SVM)\n",
    "\n",
    "You can see that the code below is almos exactly the same as it was for the linear classifier - we just replace `SGDClassifier` by `sklearn.svm.SVC`.\n",
    "\n",
    "The class `sklearn.svm.SVC` implements support vector classification. The constructor of an estimator takes as arguments the parameters of the model, but for the time being, we will consider the estimator as a black box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(gamma=0.0001, C=10.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call our estimator instance `svm_clf`. As before, **It now must be fitted to the model, that is, it must learn from the data**. This is done by passing our training set to the `fit` method. Note that we only show the *training data* to the classifer - the *test* dataset is kept secret from it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can predict new values. In particular, we can ask to the classifier to recognise all of the data in our test dataset of iris flowers, which we have not used to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_clf.predict(x_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of the above \"predictions\" are correct?  This is what we call \"accuracy\".\n",
    "\n",
    "We can use the sklearn.metrics.accuracy_score to work out the accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_test=accuracy_score(y_true=y_test, y_pred=svm_clf.predict(x_test))\n",
    "acc_train=accuracy_score(y_true=y_train, y_pred=svm_clf.predict(x_train))\n",
    "print(\"test set accuracy\",acc_test)\n",
    "print(\"train set accuracy\",acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred=svm_clf.predict(x_test))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(y_test, y_pred=svm_clf.predict(x_test))\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporing SVM hyper-parameters.\n",
    "\n",
    "When we created our SVM classifer with `svm.SVC(gamma=0.0001, C=10.)`, we observe that it has two hyper parameters, $C=10$  and $gamma=0.0001$.  These are two \"hyper-parameters\" of the SVM machine learning algorithm.  They affect the way the algorithm behaves.\n",
    "\n",
    "Remember from the lectures, $C$ specifies how much flexibility the model has in solving the task.  $gamma$ has a similar function.  We will concentrate on exploring the possible values of $C$ only below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Manual hyper-parameter search\n",
    "\n",
    "- See if you can find a better value of $C$ than 10.  Try $C=1,10,100,1000$ in turn.  Note you'd have to re-run the above 4 code blocks in sequence with each different C$ value to do this.\n",
    "\n",
    "- Plot a graph with C on the x-axis, and y-axis showing accuracy.  Do it for 2 curves, the test accuracy and the train accuracy.  \n",
    "\n",
    "- Some starter code to deal with plotting graphs is given to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list=[1,10,100,1000]\n",
    "test_accuracy_list=[0.5,0.5,0.5,0.5] # TODO update these to be your observed values\n",
    "train_accuracy_list=[0.6,0.6,0.6,0.6] # TODO update these to be your observed values\n",
    "plt.plot(c_list, test_accuracy_list, label=\"test set\")\n",
    "plt.plot(c_list, train_accuracy_list, label=\"train set\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Automation of the search for hyper-parameters\n",
    "\n",
    "Try and automate the production of the above graph using a loop.  Put your code that does everthing (including plotting the graph below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Each time you evaluate a new C, reshuffle the datasets and slplit into test+training sets.  Then for each C, calculate the average accuracy over 4 trials.  Plot your resulting graph below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
